{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot\n!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy","metadata":{"id":"1_BzYtezeSQR","outputId":"d846876d-774f-4591-f960-03232fbe502d","execution":{"iopub.status.busy":"2023-09-10T14:27:41.722783Z","iopub.execute_input":"2023-09-10T14:27:41.723160Z","iopub.status.idle":"2023-09-10T14:28:09.031759Z","shell.execute_reply.started":"2023-09-10T14:27:41.723126Z","shell.execute_reply":"2023-09-10T14:28:09.030604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the required libraries.\nimport os\nimport cv2\nimport pafy\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\nfrom moviepy.editor import *\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"id":"QRxCHsQhecv-","execution":{"iopub.status.busy":"2023-09-10T14:28:29.263563Z","iopub.execute_input":"2023-09-10T14:28:29.264360Z","iopub.status.idle":"2023-09-10T14:28:29.279468Z","shell.execute_reply.started":"2023-09-10T14:28:29.264318Z","shell.execute_reply":"2023-09-10T14:28:29.278326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"id":"5iVjakPZehmS","execution":{"iopub.status.busy":"2023-09-10T14:28:32.921501Z","iopub.execute_input":"2023-09-10T14:28:32.922679Z","iopub.status.idle":"2023-09-10T14:28:32.929392Z","shell.execute_reply.started":"2023-09-10T14:28:32.922632Z","shell.execute_reply":"2023-09-10T14:28:32.928332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install unzip","metadata":{"id":"Lf9GXSXztFAi","execution":{"iopub.status.busy":"2023-09-08T13:06:29.133741Z","iopub.execute_input":"2023-09-08T13:06:29.134166Z","iopub.status.idle":"2023-09-08T13:06:31.387110Z","shell.execute_reply.started":"2023-09-08T13:06:29.134140Z","shell.execute_reply":"2023-09-08T13:06:31.385942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Matplotlib figure and specify the size of the figure.\nplt.figure(figsize = (20, 20))\n\n# Get the names of all classes/categories in UCF50.\nall_classes_names = os.listdir('/kaggle/input/ucf50/UCF50')\n\n# Generate a list of 20 random values. The values will be between 0-50,\n# where 50 is the total number of class in the dataset.\nrandom_range = random.sample(range(len(all_classes_names)), 20)\n\n# Iterating through all the generated random values.\nfor counter, random_index in enumerate(random_range, 1):\n\n    # Retrieve a Class Name using the Random Index.\n    selected_class_Name = all_classes_names[random_index]\n\n    # Retrieve the list of all the video files present in the randomly selected Class Directory.\n    video_files_names_list = os.listdir(f'/kaggle/input/ucf50/UCF50/{selected_class_Name}')\n\n    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.\n    selected_video_file_name = random.choice(video_files_names_list)\n\n    # Initialize a VideoCapture object to read from the video File.\n    video_reader = cv2.VideoCapture(f'/kaggle/input/ucf50/UCF50/{selected_class_Name}/{selected_video_file_name}')\n\n    # Read the first frame of the video file.\n    _, bgr_frame = video_reader.read()\n\n    # Release the VideoCapture object.\n    video_reader.release()\n\n    # Convert the frame from BGR into RGB format.\n    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n\n    # Write the class name on the video frame.\n    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n\n    # Display the frame.\n    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')","metadata":{"id":"y49iqtUceiS1","outputId":"ebb31e55-edb9-4b9a-c6cd-e2dfc727dc7f","execution":{"iopub.status.busy":"2023-09-08T13:08:50.752191Z","iopub.execute_input":"2023-09-08T13:08:50.752594Z","iopub.status.idle":"2023-09-08T13:08:53.965370Z","shell.execute_reply.started":"2023-09-08T13:08:50.752562Z","shell.execute_reply":"2023-09-08T13:08:53.964223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the height and width to which each video frame will be resized in our dataset.\nIMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n\n# Specify the number of frames of a video that will be fed to the model as one sequence.\nSEQUENCE_LENGTH = 8\n\n# Specify the directory containing the UCF50 dataset.\nDATASET_DIR = \"/kaggle/input/ucf50/UCF50\"\n\n# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\nCLASSES_LIST = os.listdir('/kaggle/input/ucf50/UCF50')","metadata":{"id":"o_VkRB3_elLr","execution":{"iopub.status.busy":"2023-09-10T14:28:37.592264Z","iopub.execute_input":"2023-09-10T14:28:37.593233Z","iopub.status.idle":"2023-09-10T14:28:37.608573Z","shell.execute_reply.started":"2023-09-10T14:28:37.593197Z","shell.execute_reply":"2023-09-10T14:28:37.607600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLASSES_LIST)","metadata":{"id":"ikQ4KSMUoWOi","execution":{"iopub.status.busy":"2023-09-08T13:09:07.138913Z","iopub.execute_input":"2023-09-08T13:09:07.139269Z","iopub.status.idle":"2023-09-08T13:09:07.145571Z","shell.execute_reply.started":"2023-09-08T13:09:07.139240Z","shell.execute_reply":"2023-09-08T13:09:07.144477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = len(CLASSES_LIST)\nprint(NUM_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:28:41.709582Z","iopub.execute_input":"2023-09-10T14:28:41.710636Z","iopub.status.idle":"2023-09-10T14:28:41.716522Z","shell.execute_reply.started":"2023-09-10T14:28:41.710598Z","shell.execute_reply":"2023-09-10T14:28:41.715346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n    '''\n    This function will extract the required frames from a video after resizing and normalizing them.\n    Args:\n        video_path: The path of the video in the disk, whose frames are to be extracted.\n    Returns:\n        frames_list: A list containing the resized and normalized frames of the video.\n    '''\n\n    # Declare a list to store video frames.\n    frames_list = []\n\n    # Read the Video File using the VideoCapture object.\n    video_reader = cv2.VideoCapture(video_path)\n\n    # Get the total number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n\n    # Iterate through the Video Frames.\n    for frame_counter in range(SEQUENCE_LENGTH):\n\n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n\n        # Reading the frame from the video.\n        success, frame = video_reader.read()\n\n        # Check if Video frame is not successfully read then break the loop\n        if not success:\n            break\n\n        # Resize the Frame to fixed height and width.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n        normalized_frame = resized_frame / 255\n\n        # Append the normalized frame into the frames list\n        frames_list.append(normalized_frame)\n\n    # Release the VideoCapture object.\n    video_reader.release()\n\n    # Return the frames list.\n    return frames_list","metadata":{"id":"aTUJXZdkepAs","execution":{"iopub.status.busy":"2023-09-08T13:09:13.559620Z","iopub.execute_input":"2023-09-08T13:09:13.559986Z","iopub.status.idle":"2023-09-08T13:09:13.569599Z","shell.execute_reply.started":"2023-09-08T13:09:13.559955Z","shell.execute_reply":"2023-09-08T13:09:13.568635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset():\n    '''\n    This function will extract the data of the selected classes and create the required dataset.\n    Returns:\n        features:          A list containing the extracted frames of the videos.\n        labels:            A list containing the indexes of the classes associated with the videos.\n        video_files_paths: A list containing the paths of the videos in the disk.\n    '''\n\n    # Declared Empty Lists to store the features, labels and video file path values.\n    features = []\n    labels = []\n    video_files_paths = []\n\n    # Iterating through all the classes mentioned in the classes list\n    for class_index, class_name in enumerate(CLASSES_LIST):\n\n        # Display the name of the class whose data is being extracted.\n        print(f'Extracting Data of Class: {class_name}')\n\n        # Get the list of video files present in the specific class name directory.\n        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n\n        # Iterate through all the files present in the files list.\n        for file_name in files_list:\n\n            # Get the complete video path.\n            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n\n            # Extract the frames of the video file.\n            frames = frames_extraction(video_file_path)\n\n            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n            if len(frames) == SEQUENCE_LENGTH:\n\n                # Append the data to their repective lists.\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n\n    # Converting the list to numpy arrays\n    features = np.asarray(features)\n    labels = np.array(labels)\n\n    # Return the frames, class index, and video file path.\n    return features, labels, video_files_paths","metadata":{"id":"lP-5JMvVepv2","execution":{"iopub.status.busy":"2023-09-08T13:09:16.080552Z","iopub.execute_input":"2023-09-08T13:09:16.080924Z","iopub.status.idle":"2023-09-08T13:09:16.089378Z","shell.execute_reply.started":"2023-09-08T13:09:16.080894Z","shell.execute_reply":"2023-09-08T13:09:16.088460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset.\nfeatures, labels, video_files_paths = create_dataset()\nnp.save('saved_features',features)\nnp.save('saved_labels',labels)\nnp.save('saved_video_files_paths',video_files_paths)","metadata":{"id":"qzzKV3L3eteR","outputId":"e0e63de7-be9a-454e-ea57-8a730892f50b","execution":{"iopub.status.busy":"2023-09-08T13:09:21.325732Z","iopub.execute_input":"2023-09-08T13:09:21.326100Z","iopub.status.idle":"2023-09-08T13:15:17.111587Z","shell.execute_reply.started":"2023-09-08T13:09:21.326071Z","shell.execute_reply":"2023-09-08T13:15:17.110350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = np.load('saved_features.npy')\nlabels = np.load('saved_labels.npy')\nvideo_files_paths = np.load('saved_video_files_paths.npy')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:10:32.850560Z","iopub.execute_input":"2023-09-08T15:10:32.850951Z","iopub.status.idle":"2023-09-08T15:10:33.535081Z","shell.execute_reply.started":"2023-09-08T15:10:32.850916Z","shell.execute_reply":"2023-09-08T15:10:33.533753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.utils import to_categorical","metadata":{"id":"f1oNfmBMp8Iu","execution":{"iopub.status.busy":"2023-09-08T14:00:41.481889Z","iopub.execute_input":"2023-09-08T14:00:41.482283Z","iopub.status.idle":"2023-09-08T14:00:41.488708Z","shell.execute_reply.started":"2023-09-08T14:00:41.482252Z","shell.execute_reply":"2023-09-08T14:00:41.487632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"id":"GCmMyfOYsVyL","execution":{"iopub.status.busy":"2023-09-08T14:00:43.817240Z","iopub.execute_input":"2023-09-08T14:00:43.817797Z","iopub.status.idle":"2023-09-08T14:00:43.825877Z","shell.execute_reply.started":"2023-09-08T14:00:43.817758Z","shell.execute_reply":"2023-09-08T14:00:43.824671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Data into Train ( 75% ) and Test Set ( 25% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,\n                                                                            test_size = 0.25, shuffle = True,\n                                                                            random_state = seed_constant)\n\nnp.save('saved_features_train',features_train)\nnp.save('saved_features_test',features_test)\nnp.save('saved_labels_train',labels_train)\nnp.save('saved_labels_test',labels_test)","metadata":{"id":"AmR2czK4exoY","outputId":"c14420a2-f99f-4475-b65e-5d23e0be3f94","execution":{"iopub.status.busy":"2023-09-08T14:01:02.126510Z","iopub.execute_input":"2023-09-08T14:01:02.127485Z","iopub.status.idle":"2023-09-08T14:01:29.109655Z","shell.execute_reply.started":"2023-09-08T14:01:02.127441Z","shell.execute_reply":"2023-09-08T14:01:29.108076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_train = np.load('/kaggle/input/features/saved_features_train.npy')\nfeatures_test = np.load('/kaggle/input/features/saved_features_test.npy')\nlabels_train = np.load('/kaggle/input/features/saved_labels_train.npy')\nlabels_test = np.load('/kaggle/input/features/saved_labels_test.npy')","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:28:47.902756Z","iopub.execute_input":"2023-09-10T14:28:47.903154Z","iopub.status.idle":"2023-09-10T14:29:17.507484Z","shell.execute_reply.started":"2023-09-10T14:28:47.903120Z","shell.execute_reply":"2023-09-10T14:29:17.506427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_trains.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:20:48.932277Z","iopub.execute_input":"2023-09-08T15:20:48.932649Z","iopub.status.idle":"2023-09-08T15:20:48.941019Z","shell.execute_reply.started":"2023-09-08T15:20:48.932612Z","shell.execute_reply":"2023-09-08T15:20:48.939972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:25:23.591425Z","iopub.execute_input":"2023-09-08T15:25:23.591816Z","iopub.status.idle":"2023-09-08T15:25:23.599267Z","shell.execute_reply.started":"2023-09-08T15:25:23.591765Z","shell.execute_reply":"2023-09-08T15:25:23.598293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:25:26.383222Z","iopub.execute_input":"2023-09-08T15:25:26.383641Z","iopub.status.idle":"2023-09-08T15:25:26.390966Z","shell.execute_reply.started":"2023-09-08T15:25:26.383608Z","shell.execute_reply":"2023-09-08T15:25:26.389827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:25:32.057446Z","iopub.execute_input":"2023-09-08T15:25:32.057856Z","iopub.status.idle":"2023-09-08T15:25:32.064314Z","shell.execute_reply.started":"2023-09-08T15:25:32.057817Z","shell.execute_reply":"2023-09-08T15:25:32.063331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow-addons","metadata":{"id":"NKDfAd6Mt29d","execution":{"iopub.status.busy":"2023-09-10T14:10:25.692305Z","iopub.execute_input":"2023-09-10T14:10:25.693351Z","iopub.status.idle":"2023-09-10T14:10:37.194366Z","shell.execute_reply.started":"2023-09-10T14:10:25.693315Z","shell.execute_reply":"2023-09-10T14:10:37.193103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install keras-tcn","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:07:07.519519Z","iopub.execute_input":"2023-09-09T08:07:07.519966Z","iopub.status.idle":"2023-09-09T08:07:20.328244Z","shell.execute_reply.started":"2023-09-09T08:07:07.519928Z","shell.execute_reply":"2023-09-09T08:07:20.327047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-addons==0.16.1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T06:21:47.102913Z","iopub.execute_input":"2023-09-09T06:21:47.103924Z","iopub.status.idle":"2023-09-09T06:22:02.168434Z","shell.execute_reply.started":"2023-09-09T06:21:47.103875Z","shell.execute_reply":"2023-09-09T06:22:02.167270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2023-09-09T06:23:04.587568Z","iopub.execute_input":"2023-09-09T06:23:04.587995Z","iopub.status.idle":"2023-09-09T06:23:04.593085Z","shell.execute_reply.started":"2023-09-09T06:23:04.587932Z","shell.execute_reply":"2023-09-09T06:23:04.591780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, GlobalAveragePooling3D, Dense, Flatten\n#from tcn import TCN\n#from tensorflow_addons.layers import TimeDistributedNormalization","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:29:34.873636Z","iopub.execute_input":"2023-09-10T14:29:34.874016Z","iopub.status.idle":"2023-09-10T14:29:34.879600Z","shell.execute_reply.started":"2023-09-10T14:29:34.873985Z","shell.execute_reply":"2023-09-10T14:29:34.878480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the input shape (adjust this according to your data)\ninput_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)  # Replace with actual values\n\n# Create the input layer\ninput_tensor = Input(shape=input_shape)\n\n# Define the I3D pathway\nconv1 = Conv3D(64, (7, 7, 7), strides=(2, 2, 2), padding='same', activation='relu')(input_tensor)\nmaxpool1 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(conv1)\n\nconv2 = Conv3D(64, (1, 1, 1), padding='same', activation='relu')(maxpool1)\nconv3 = Conv3D(192, (3, 3, 3), padding='same', activation='relu')(conv2)\nmaxpool2 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(conv3)\n\n# I3D pathway's Global Average Pooling\ni3d_gap = GlobalAveragePooling3D()(maxpool2)\n\n# Define the TCN pathway\ntcn_conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(input_tensor)\ntcn_conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(tcn_conv1)\npooling = MaxPooling3D((2, 2, 2))(tcn_conv2)\n\nflattened = Flatten()(pooling)\nfc1 = Dense(128, activation='relu')(flattened)\noutput_tcn = Dense(NUM_CLASSES, activation='softmax')(fc1)\n#tcn_gap = GlobalAveragePooling3D()(output_tcn)\n\n# Combine features from both pathways\ncombined_features = tf.keras.layers.concatenate([i3d_gap, output_tcn])\n\n# Fully connected layers\nfc2 = Dense(512, activation='relu')(combined_features)\noutput = Dense(NUM_CLASSES, activation='softmax')(fc2)  # Replace NUM_CLASSES with your number of activity classes\n\n# Create the model\nmodel = tf.keras.Model(inputs=input_tensor, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"id":"h1oGFm40t6xL","execution":{"iopub.status.busy":"2023-09-08T15:25:44.222346Z","iopub.execute_input":"2023-09-08T15:25:44.223545Z","iopub.status.idle":"2023-09-08T15:25:49.068167Z","shell.execute_reply.started":"2023-09-08T15:25:44.223496Z","shell.execute_reply":"2023-09-08T15:25:49.067353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model,show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:19:00.316129Z","iopub.execute_input":"2023-09-08T15:19:00.316511Z","iopub.status.idle":"2023-09-08T15:19:00.665931Z","shell.execute_reply.started":"2023-09-08T15:19:00.316479Z","shell.execute_reply":"2023-09-08T15:19:00.664852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(features_trains, labels_train, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:25:57.152279Z","iopub.execute_input":"2023-09-08T15:25:57.152751Z","iopub.status.idle":"2023-09-08T15:40:43.502859Z","shell.execute_reply.started":"2023-09-08T15:25:57.152713Z","shell.execute_reply":"2023-09-08T15:40:43.501733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('1st_model.keras')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T15:45:17.340128Z","iopub.execute_input":"2023-09-08T15:45:17.340533Z","iopub.status.idle":"2023-09-08T15:45:18.912323Z","shell.execute_reply.started":"2023-09-08T15:45:17.340502Z","shell.execute_reply":"2023-09-08T15:45:18.911055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Inception3D(input_tensor):\n    # Define the Inception module\n    # You can customize the filters and kernel sizes based on your needs\n    conv1x1 = Conv3D(64, (1, 1, 1), padding='same', activation='relu')(input_tensor)\n    conv3x3_reduce = Conv3D(96, (1, 1, 1), padding='same', activation='relu')(input_tensor)\n    conv3x3 = Conv3D(128, (3, 3, 3), padding='same', activation='relu')(conv3x3_reduce)\n    conv5x5_reduce = Conv3D(16, (1, 1, 1), padding='same', activation='relu')(input_tensor)\n    conv5x5 = Conv3D(32, (5, 5, 5), padding='same', activation='relu')(conv5x5_reduce)\n    maxpool = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same')(input_tensor)\n    conv1x1_proj = Conv3D(32, (1, 1, 1), padding='same', activation='relu')(maxpool)\n    inception_output = tf.keras.layers.concatenate([conv1x1, conv3x3, conv5x5, conv1x1_proj], axis=-1)\n    return inception_output","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:18.402647Z","iopub.execute_input":"2023-09-10T14:30:18.403931Z","iopub.status.idle":"2023-09-10T14:30:18.412475Z","shell.execute_reply.started":"2023-09-10T14:30:18.403882Z","shell.execute_reply":"2023-09-10T14:30:18.411486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n\ninput_tensor = Input(shape=input_shape)\n\n#I3D Pathway\n\nI3D_conv1 = Conv3D(64, (7, 7, 7), strides=(2, 2, 2), padding='same', activation='relu')(input_tensor)\nI3D_maxpool1 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(I3D_conv1)\n\nI3D_conv2 = Conv3D(64, (1, 1, 1), padding='same', activation='relu')(I3D_maxpool1)\nI3D_conv3 = Conv3D(192, (3, 3, 3), padding='same', activation='relu')(I3D_conv2)\nI3D_maxpool2 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(I3D_conv3)\n\nI3D_inception3a = Inception3D(I3D_maxpool2)\nI3D_inception3b = Inception3D(I3D_inception3a)\nI3D_gap = GlobalAveragePooling3D()(I3D_inception3b)\n\n#TCN Pathway\n\ntcn_conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(input_tensor)\ntcn_conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(tcn_conv1)\ntcn_pooling = MaxPooling3D((2, 2, 2))(tcn_conv2)\n\ntcn_flattened = Flatten()(tcn_pooling)\n#tcn_fc = Dense(128, activation='relu')(tcn_flattened)\ntcn_gap = GlobalAveragePooling3D()(tcn_pooling)\n\n#Concatenated Features\ncombined_features = tf.keras.layers.concatenate([I3D_gap, tcn_gap])\n\n#Fully connected layer\n\nfc1 = Dense(512, activation='relu')(combined_features)\noutput = Dense(NUM_CLASSES, activation='softmax')(fc1)\n\nmodel_com = tf.keras.Model(inputs=input_tensor, outputs=output)\n\nmodel_com.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel_com.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:07:37.156704Z","iopub.execute_input":"2023-09-09T08:07:37.157099Z","iopub.status.idle":"2023-09-09T08:07:42.610236Z","shell.execute_reply.started":"2023-09-09T08:07:37.157067Z","shell.execute_reply":"2023-09-09T08:07:42.609407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_com.fit(features_train, labels_train, epochs=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:07:51.452513Z","iopub.execute_input":"2023-09-09T08:07:51.452934Z","iopub.status.idle":"2023-09-09T08:31:23.101606Z","shell.execute_reply.started":"2023-09-09T08:07:51.452902Z","shell.execute_reply":"2023-09-09T08:31:23.100475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting the loss curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['loss'], 'black', linewidth=2.0)\n#plt.plot(history.history['val_loss'], 'blue', linewidth=2.0)\nplt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Loss', fontsize=10)\nplt.title('Loss Curves', fontsize=12)\n\n# Plotting the accuracy curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n#plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:31:28.853646Z","iopub.execute_input":"2023-09-09T08:31:28.854041Z","iopub.status.idle":"2023-09-09T08:31:29.772338Z","shell.execute_reply.started":"2023-09-09T08:31:28.854009Z","shell.execute_reply":"2023-09-09T08:31:29.771268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_com.save('I3d_TCN_model.keras')","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:32:47.036935Z","iopub.execute_input":"2023-09-09T08:32:47.037472Z","iopub.status.idle":"2023-09-09T08:32:47.226702Z","shell.execute_reply.started":"2023-09-09T08:32:47.037433Z","shell.execute_reply":"2023-09-09T08:32:47.225590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model_com, to_file=\"iris_model.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:42:24.931076Z","iopub.execute_input":"2023-09-10T13:42:24.931495Z","iopub.status.idle":"2023-09-10T13:42:25.422989Z","shell.execute_reply.started":"2023-09-10T13:42:24.931460Z","shell.execute_reply":"2023-09-10T13:42:25.421706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Conv1D, BatchNormalization, Activation","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:27.106417Z","iopub.execute_input":"2023-09-10T14:30:27.107147Z","iopub.status.idle":"2023-09-10T14:30:27.111714Z","shell.execute_reply.started":"2023-09-10T14:30:27.107109Z","shell.execute_reply":"2023-09-10T14:30:27.110695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TemporalConvNet(Layer):\n    def __init__(self, num_filters, kernel_size, dilations):\n        super(TemporalConvNet, self).__init__()\n        self.num_filters = num_filters\n        self.kernel_size = kernel_size\n        self.dilations = dilations\n        #self.use_batch_norm = use_batch_norm\n        self.conv_layers = []\n\n        for dilation in dilations:\n            conv = Conv1D(filters=num_filters,\n                          kernel_size=kernel_size,\n                          dilation_rate=dilation,\n                          padding='causal',\n                          activation='relu',\n                          kernel_initializer='he_normal')\n\n            self.conv_layers.append(conv)\n\n    def call(self, inputs):\n        x = inputs\n        for layer in self.conv_layers:\n            x = layer(x)\n            #if self.use_batch_norm:\n                #x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:31.025694Z","iopub.execute_input":"2023-09-10T14:30:31.026063Z","iopub.status.idle":"2023-09-10T14:30:31.035489Z","shell.execute_reply.started":"2023-09-10T14:30:31.026031Z","shell.execute_reply":"2023-09-10T14:30:31.034389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define TCN parameters\nnum_filters = 32\nkernel_size = 3\ndilations = [1, 2, 4]","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:34.884754Z","iopub.execute_input":"2023-09-10T14:30:34.885574Z","iopub.status.idle":"2023-09-10T14:30:34.894193Z","shell.execute_reply.started":"2023-09-10T14:30:34.885533Z","shell.execute_reply":"2023-09-10T14:30:34.893142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n\ninput_tensor = Input(shape=input_shape)\n\n#I3D Pathway\n\nI3D_conv1 = Conv3D(64, (7, 7, 7), strides=(2, 2, 2), padding='same', activation='relu')(input_tensor)\nI3D_maxpool1 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(I3D_conv1)\n\nI3D_conv2 = Conv3D(64, (1, 1, 1), padding='same', activation='relu')(I3D_maxpool1)\nI3D_conv3 = Conv3D(192, (3, 3, 3), padding='same', activation='relu')(I3D_conv2)\nI3D_maxpool2 = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same')(I3D_conv3)\n\nI3D_inception3a = Inception3D(I3D_maxpool2)\nI3D_inception3b = Inception3D(I3D_inception3a)\nI3D_gap = GlobalAveragePooling3D()(I3D_inception3b)\n\n\n# Create TCN layer\ntcn_layer_1 = TemporalConvNet(num_filters, kernel_size, dilations)(input_tensor)\ntcn_layer_1 = tf.keras.layers.BatchNormalization()(tcn_layer_1)\ntcn_layer_2 = TemporalConvNet(num_filters, kernel_size, dilations)(tcn_layer_1)\ntcn_layer_2 = tf.keras.layers.BatchNormalization()(tcn_layer_2)\ntcn_gap = GlobalAveragePooling3D()(tcn_layer_2)\n\n#Concatenated Features\ncombined_features = tf.keras.layers.concatenate([I3D_gap, tcn_gap])\n\n#Fully connected layer\n\nfc1 = Dense(512, activation='relu')(combined_features)\noutput = Dense(NUM_CLASSES, activation='softmax')(fc1)\n\nmodel_com = tf.keras.Model(inputs=input_tensor, outputs=output)\n\nmodel_com.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel_com.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:44.468483Z","iopub.execute_input":"2023-09-10T14:30:44.468869Z","iopub.status.idle":"2023-09-10T14:30:48.041947Z","shell.execute_reply.started":"2023-09-10T14:30:44.468838Z","shell.execute_reply":"2023-09-10T14:30:48.041168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model_com, to_file=\"iris_model.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:27:08.109095Z","iopub.execute_input":"2023-09-10T14:27:08.109829Z","iopub.status.idle":"2023-09-10T14:27:08.521976Z","shell.execute_reply.started":"2023-09-10T14:27:08.109794Z","shell.execute_reply":"2023-09-10T14:27:08.521073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_com.fit(features_train, labels_train, epochs=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:30:55.481836Z","iopub.execute_input":"2023-09-10T14:30:55.482222Z","iopub.status.idle":"2023-09-10T14:45:28.093244Z","shell.execute_reply.started":"2023-09-10T14:30:55.482189Z","shell.execute_reply":"2023-09-10T14:45:28.092019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_com.save('Corrected_model.keras')","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:45:59.191930Z","iopub.execute_input":"2023-09-10T14:45:59.192328Z","iopub.status.idle":"2023-09-10T14:45:59.480862Z","shell.execute_reply.started":"2023-09-10T14:45:59.192294Z","shell.execute_reply":"2023-09-10T14:45:59.479466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting the loss curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['loss'], 'black', linewidth=2.0)\n#plt.plot(history.history['val_loss'], 'blue', linewidth=2.0)\nplt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Loss', fontsize=10)\nplt.title('Loss Curves', fontsize=12)\n\n# Plotting the accuracy curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n#plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:46:38.963025Z","iopub.execute_input":"2023-09-10T14:46:38.963697Z","iopub.status.idle":"2023-09-10T14:46:39.722594Z","shell.execute_reply.started":"2023-09-10T14:46:38.963651Z","shell.execute_reply":"2023-09-10T14:46:39.721637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}